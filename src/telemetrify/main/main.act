# Copyright (C) Deutsche Telekom AG
#
# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#

import logging
import http
import net
import time
import xml

import telemetrify.net.netconf as netconf
import telemetrify.nso.subscriber
import telemetrify.nso.writer
import telemetrify.nsoapi.cdb
import telemetrify.nsoapi.maapi
import telemetrify.nsoapi.schema as schema
import telemetrify.tsdb.writer
import telemetrify.ietf.l3vpn_svc
import telemetrify.ietf.cat8k
import tsdb.m3

from telemetrify.common.mod import *
from telemetrify.common.utils import *
from telemetrify.nso.subscriber import *
from telemetrify.nso.writer import *
from telemetrify.nsoapi.conf import *
from telemetrify.nsoapi.conv import *
from telemetrify.nsoapi.cdb import *
from telemetrify.nsoapi.maapi import *
from telemetrify.nsoapi.schema import QName, SchemaPath
from telemetrify.main.config import *
import telemetrify.vmanage.vmanage as vmanage
import telemetrify.vmanage.transform

# class Poll(object):
#     pass

_SUBSCRIPTION_TYPE_ACTION_POLLER = PTag('tlm', 'netconf-rpc-poll')
_SUBSCRIPTION_TYPE_VMANAGE_POLLER = PTag('tlm', 'vmanage-poll')
_SUBSCRIPTION_TYPE_GET_POLLER = PTag('tlm', 'netconf-get-poll')

def tnode_empty() -> TNode:
    return TNode(OP_NONE, PTag.root())

def tnode_root() -> TNode:
    return TTree(OP_NONE, PTag.root(), None, {})

class Sink(object):
    @property
    write_fn: action(Node, TNode, TNode) -> None
    @property
    close_fn: action() -> None

    def __init__(
            self,
            write_fn: action(Node, TNode, TNode) -> None,
            close_fn: action() -> None,
            ):
        self.write_fn = write_fn
        self.close_fn = close_fn

    def write(self, node: Node, source_params: TNode, sink_config: TNode) -> None:
        self.write_fn(node, source_params, sink_config)

    def close(self):
        self.close_fn()

class Source(object):
    @property
    update_config_fn: action(?TNode, dict[Keypath, SubscriptionUpdate]) -> None
    @property
    close_fn: action() -> None
    @property
    subscription_keys: set[Keypath]

    def __init__(
            self,
            key: Keypath,
            update_config_fn: action(?TNode, dict[Keypath, SubscriptionUpdate]) -> None,
            close_fn: action() -> None
            ):
        self.key = key
        self.update_config_fn = update_config_fn
        self.close_fn = close_fn
        self.subscription_keys = set([])

    def has_subscriptions(self) -> bool:
        return bool(self.subscription_keys)

    def update_config(self, source_update: ?TNode, subscription_updates: dict[Keypath, SubscriptionUpdate]):
        for dev_sub_key, sub_update in subscription_updates.items():
            #if sub_update.config is not None:
            _config = sub_update.config
            if _config is not None:
                self.subscription_keys.add(dev_sub_key)
            else:
                self.subscription_keys.discard(dev_sub_key)

        self.update_config_fn(source_update, subscription_updates)

    def close(self):
        self.close_fn()

class Subscriber(object):
    @property
    update_config_fn: action(SubscriptionUpdate) -> None
    @property
    start_fn: action() -> None
    @property
    stop_fn: action() -> None
    @property
    close_fn: action() -> None

    def __init__(
            self,
            update_config_fn: action(SubscriptionUpdate) -> None,
            start_fn: action() -> None,
            stop_fn: action() -> None,
            close_fn: action() -> None
            ):
        self.update_config_fn = update_config_fn
        self.start_fn = start_fn
        self.stop_fn = stop_fn
        self.close_fn = close_fn

    def update_config(self, update: SubscriptionUpdate):
        self.update_config_fn(update)

    def start(self):
        self.start_fn()

    def stop(self):
        self.stop_fn()

    def close(self):
        self.close_fn()

class Transform(object):
    @property
    post_fn: action(Node, TNode) -> None
    @property
    close_fn: action() -> None

    def __init__(
            self,
            post_fn: action(Node, TNode) -> None,
            close_fn: action() -> None,
            ):
        self.post_fn = post_fn
        self.close_fn = close_fn

    def post(self, node: Node, source_params: TNode) -> None:
        self.post_fn(node, source_params)

    def close(self):
        self.close_fn()

class SubscriptionSink(object):
    @property
    config: TNode # TODO: Not really needed here
    @property
    sink: Sink # TODO: Not really needed here
    @property
    transforms: list[Transform]
    @property
    head_cb: action(Node, TNode) -> None

    def __init__(self, config: TNode, sink: Sink, transforms: list[Transform], head_cb: action(Node, TNode) -> None):
        self.config = config
        self.sink = sink
        self.transforms = transforms
        self.head_cb = head_cb

    def __str__(self) -> str:
        return "SubscriptionSink(" + str(self.config) + ", " + str(self.sink) + ")"

    def write(self, node: Node, source_params: TNode) -> None:
        self.head_cb(node, source_params)

    def close(self):
        # Close transforms but NOT sink as it's owned by DeviceStreamer
        for transform in self.transforms:
            transform.close()
        self.transforms.clear()

class SubscriptionSinkCollection(object):
    @property
    sub_sinks: dict[Keypath, SubscriptionSink]

    def __init__(self):
        self.sub_sinks = {}

    def update(self, sink_updates: list[(Keypath, ?SubscriptionSink)]):
        for sink_key, sink in sink_updates:
            if sink is not None:
                self.sub_sinks[sink_key] = sink
                # TODO: Signal resync where needed for differential/delta subscriptions?
            else:
                try_pop(self.sub_sinks, sink_key)

    def write(self, node: Node, source_params: TNode) -> None:
        for sub_sink in self.sub_sinks.values():
            sub_sink.write(node, source_params)

    def close(self):
        for sub_sink in self.sub_sinks.values():
            sub_sink.close()
        #self.sub_sinks.clear()
        self.sub_sinks = {}

actor NsoCdbSink(env: Env, config: TNode, shared_schema: schema.SharedSchema):
    var maapi_connection: ?telemetrify.nsoapi.maapi.MaapiConnection = None
    var writer: ?telemetrify.nso.writer.MaapiWriter = None

    var tasks: Wardrobe[(Node, TNode, TNode)] = Wardrobe() # (node, source_params, sink_config)

    print("SINK nso-cdb CREATED")

    def write(node: Node, source_params: TNode, sink_config: TNode) -> None:
        m = maapi_connection
        w = writer
        if m is not None and w is not None:
            task_id = tasks.put((node, source_params, sink_config))
            m.start_trans(DB_OPERATIONAL, MODE_READ_WRITE, UserIdentity(None, None, None, None), lambda c, t: _on_start_trans(c, t, task_id))
        else:
            # TODO:
            # Buffer until (re-)connected or signal connection upwards for possible source subscription sync?
            pass

    def _on_start_trans(_m, txn_handle, task_id):
        if isinstance(txn_handle, int):
            print("SINK nso-cdb MAAPI started transaction txn_handle:", txn_handle)
            w = writer
            if w is not None:
                try:
                    node, source_params, sink_config = tasks.borrow(task_id)
                except Exception:
                    pass
                else:
                    w.write(node, txn_handle, False, lambda e: _on_write(e, txn_handle, task_id))
        else:
            print("SINK nso-cdb MAAPI start transaction failed:", txn_handle)

    def _on_write(e: ?Exception, txn_handle: int, task_id: int):
        if e is not None:
            print("SINK nso-cdb MAAPI write txn_handle:", txn_handle, "failed:", e)
        m = maapi_connection
        if m is not None:
            m.apply_trans(txn_handle, False, 0, lambda _c, e: _on_apply_trans(e, txn_handle, task_id))

    def _on_apply_trans(e: ?Exception, txn_handle: int, task_id: int):
        if e is not None:
            print("SINK nso-cdb MAAPI apply transaction txn_handle:", txn_handle, "failed:", e)
        m = maapi_connection
        if m is not None:
            m.finish_trans(txn_handle, lambda _c, e: _on_finish_trans(e, txn_handle, task_id))

    def _on_finish_trans(e: ?Exception, txn_handle: int, task_id: int):
        if e is not None:
            print("SINK nso-cdb MAAPI finish transaction txn_handle:", txn_handle, "failed:", e)
        else:
            print("SINK nso-cdb WRITE DONE")
        try:
            tasks.discard(task_id)
        except Exception:
            pass

    def _on_maapi_connect(_m: MaapiConnection):
        print("SINK nso-cdb MAAPI connect complete")
        m = maapi_connection
        if m is not None:
            m.start_user_session(UserSessionDescription("admin", "127.0.0.1", "system", [], None, False, UserIdentity(None, None, None, None)), _on_user_session)

    def _on_user_session(c, e):
        m = maapi_connection
        if m is not None:
            writer = telemetrify.nso.writer.MaapiWriter(m, shared_schema)

    def _on_maapi_connect_error(e):
        print("SINK nso-cdb MAAPI connect failed:", e)
        # TODO: retry? We should probably panic/exit/crash if we don't have a local NSO

    def close():
        _writer = writer
        if _writer is not None:
            # TODO
            # _writer.close()
            _writer = None
        _maapi_connection = maapi_connection
        if _maapi_connection is not None:
            _maapi_connection.close()
            maapi_connection = None
        tasks.clear()
        print("SINK nso-cdb CLOSED")

    maapi_connection = telemetrify.nsoapi.maapi.MaapiConnection(env, 4569, _on_maapi_connect, _on_maapi_connect_error, None)

actor M3dbSink(auth: WorldCap, config: TNode, shared_schema: schema.SharedSchema, log_handler: logging.Handler):
    var logh = logging.Handler("m3db-sink")
    if log_handler is not None:
        logh.set_handler(log_handler)

    var log = logging.Logger(logh)

    config_m3db = config[PTag('tlm', 'm3db')]
    address = config_m3db[PTag('tlm', 'address')].req_str()
    port = config_m3db[PTag('tlm', 'port')].req_int()

    var client: ?tsdb.m3.Client = None
    var writer: ?telemetrify.tsdb.writer.TSDBWriter = None

    log.info("SINK m3db CREATED", None)

    def write(node: Node, source_params: TNode, sink_config: TNode) -> None:
        w = writer
        if w is not None:
            _schema_path: ?value = source_params[PTag(None, 'schema_path')].value()
            schema_path: ?SchemaPath = _schema_path if _schema_path is not None and isinstance(_schema_path, SchemaPath) else None
            _timestamp: ?value = source_params[PTag(None, 'timestamp')].value()
            timestamp: time.Instant = _timestamp if _timestamp is not None and isinstance(_timestamp, time.Instant) else time.time()
            base_tags: list[(str, str)] = []
            for tag_entry in sink_config[PTag(None, 'base_tags')].iter():
                _tag = tag_entry[PTag(None, 'name')].try_str()
                _value = tag_entry[PTag(None, 'value')].try_str()
                if _tag is not None and _value is not None:
                    base_tags.append((_tag, _value))

            m3db_timestamp = timestamp.second # TODO: Subsecond precision
            # TODO:
            # Why: Cannot unify ?T_619 and telemetrify.common.mod.SchemaPath
            # when argment is ?SchemaPath ?
            #w.write(node, schema_path, [], base_tags, m3db_timestamp, _on_write_done)
            w.write(node, schema_path if schema_path is not None else SchemaPath([], []), base_tags, m3db_timestamp, _on_write_done)
        else:
            # TODO:
            # Buffer until (re-)connected or signal connection upwards for possible source subscription sync?
            # Not really relevant for periodic time-series ...
            pass
            # # <TEST>
            # _schema_path: ?value = source_params[PTag(None, 'schema_path')].value()
            # schema_path: ?SchemaPath = _schema_path if _schema_path is not None and isinstance(_schema_path, SchemaPath) else None
            # _timestamp: ?value = source_params[PTag(None, 'timestamp')].value()
            # timestamp: time.Instant = _timestamp if _timestamp is not None and isinstance(_timestamp, time.Instant) else time.time()
            # base_tags: list[(str, str)] = []
            # for tag_entry in sink_config[PTag(None, 'base_tags')].iter():
            #     _tag = tag_entry[PTag(None, 'name')].try_str()
            #     _value = tag_entry[PTag(None, 'value')].try_str()
            #     if _tag is not None and _value is not None:
            #         base_tags.append((_tag, _value))
            # m3db_timestamp = timestamp.second # TODO: Subsecond precision
            # print("SINK m3db WRITE Attempted", node, optional_str(schema_path, "None"), base_tags, m3db_timestamp)
            # # </TEST>

    def close():
        # TODO: Flush writer?
        # _writer = writer
        # if _writer is not None:
        #     _writer.flush()
        _client = client
        if _client is not None:
            # TODO:
            # _client.close()
            pass

        log.info("SINK m3db CLOSED", None)

    def _on_tsdb_connect(c: tsdb.m3.Client):
        client = c
        c.quick_init(_on_tsdb_init)
        log.info("SINK m3db CONNECTED", None)

    def _on_tsdb_init(c: tsdb.m3.Client, success: bool):
        writer = telemetrify.tsdb.writer.TSDBWriter(c, shared_schema)
        print("SINK m3db INITIALIZED")

    def _on_tsdb_error(c: tsdb.m3.Client, e):
        log.error("SINK m3db CLIENT ERROR:", {"error": str(e)})
        # TODO: Reconnect ourselves or pass along upwards?

    def _on_write_done(c, e):
        if e is not None:
            log.error("SINK m3db WRITE ERROR:", {"error": str(e)})
        else:
            log.info("SINK m3db WRITE DONE", None)

    #log_handler.add_sink(logging.StdoutSink())

    #log = logging.Logger(log_handler)

    tsdb.m3.Client(net.TCPConnectCap(net.TCPCap(net.NetCap(auth))), address, port, _on_tsdb_connect, _on_tsdb_error, logh)

#actor NetconfSource(auth: WorldCap, on_ready: action(), on_error: action(str) -> None, on_closed() -> None):
actor NetconfSource(auth: WorldCap, shared_schema: schema.SharedSchema, log_handler: logging.Handler):
    var logh = logging.Handler("netconf-source")
    if log_handler is not None:
        logh.set_handler(log_handler)

    var log = logging.Logger(logh)

    var config: TNode = tnode_empty()
    var client: ?netconf.Client = None
    var client_seqno: int = 0
    var subscribers: dict[Keypath, Subscriber] = {}

    log.info("SOURCE netconf CREATED", None)

    def _on_client_connect(seqno: int):
        if seqno != client_seqno:
            return
        _start_subscribers()

    def _on_client_error(error_msg: str, seqno: int):
        if seqno != client_seqno:
            return
        _close_client()
        # TODO: Configurable retry timer/backoff?
        after 1: _connect_client()

    def _on_client_notif(node: xml.Node, seqno: int):
        # if seqno != client_seqno:
        #     return
        pass

    def update_config(source_update: ?TNode, subscription_updates: dict[Keypath, SubscriptionUpdate]):
        log.debug("SOURCE netconf CONFIG update", {"source_update": optional_str(source_update, "None")})
        log.debug("SOURCE netconf SUBSCRIPTION update", {"subscription_updates": mapping_str(subscription_updates)})

        restart = False

        if source_update is not None:
            if _close_client():
                restart = True

            config = source_update

            _connect_client()

        for dev_sub_key, sub_update in subscription_updates.items():
            if sub_update is not None:
                log.trace("SOURCE netconf SUBSCRIPTION update", {"sub_update": sub_update})
                sub = try_get(subscribers, dev_sub_key)
                if sub is not None:
                    log.trace("SOURCE netconf SUBSCRIPTION update exists", {"sub_update": sub_update})
                    sub.update_config(sub_update)
                else:
                    log.trace("SOURCE netconf SUBSCRIPTION update is new", {"sub_update": sub_update})
                    new_sub = _create_subscriber(dev_sub_key, sub_update)
                    new_sub.update_config(sub_update)
                    log.trace("SOURCE netconf SUBSCRIPTION update 'restart'", {"restart": restart})
                    log.trace("SOURCE netconf SUBSCRIPTION update 'client'", {"client": optional_str(client, "None")})
                    if not restart and client is not None:
                        log.debug("SOURCE netconf SUBSCRIPTION update to start", {"sub_update": sub_update})
                        new_sub.start()
                    subscribers[dev_sub_key] = new_sub
            else:
                sub = try_pop(subscribers, dev_sub_key)
                if sub is not None:
                    sub.close()

        if restart:
            _start_subscribers()

    def _create_subscriber(dev_sub_key: Keypath, sub_update: SubscriptionUpdate) -> Subscriber:
        sub_config = sub_update.config
        if sub_config is not None:
            if sub_config[_SUBSCRIPTION_TYPE_ACTION_POLLER].exists():
                sub_act = NetconfRpcPollSubscription(self, dev_sub_key, shared_schema, logh)
                return Subscriber(sub_act.update_config, sub_act.start, sub_act.stop, sub_act.close)
            elif sub_config[_SUBSCRIPTION_TYPE_GET_POLLER].exists():
                sub_act = NetconfGetPollSubscription(self, dev_sub_key, shared_schema, None)
                return Subscriber(sub_act.update_config, sub_act.start, sub_act.stop, sub_act.close)

        raise Exception("Broken invariant: Attempted to create subscriber from invalid config: " + optional_str(sub_config, "None"))

    def _connect_client():
        address = config[PTag('tlm', 'address')].try_str()
        port = config[PTag('tlm', 'port')].try_int()
        username = config[PTag('tlm', 'username')].try_str()
        password = config[PTag('tlm', 'password')].try_str()
        key: ?str = None # TODO
        log.debug("SOURCE netconf CONNECT", {"address": optional_str(address, "None"), "port": optional_str(port, "None"), "username": optional_str(username, "None"), "password": optional_str(password, "None")})

        if address is not None and port is not None and \
                username is not None and (password is not None or key is not None):

            client_seqno += 1

            def __on_client_connect(_c):
                _on_client_connect(client_seqno)
            def __on_client_error(_c, e):
                _on_client_error(e, client_seqno)
            def __on_client_notif(_c, n):
                _on_client_notif(n, client_seqno)

            new_client = netconf.Client(auth, address, port, username, password, key,
                # lambda _c: _on_client_connect(client_seqno),
                __on_client_connect,
                # lambda _c, e: _on_client_error(e, client_seqno),
                __on_client_error,
                # lambda _c, n: _on_client_notif(n, client_seqno))
                __on_client_notif)

            client = new_client

        # # <TEST>
        # after 0: _start_subscribers()
        # # </TEST>

    def _close_client() -> bool:
        _client = client
        if _client is not None:
            _stop_subscribers()
            # TODO: Gather stop replies from subscribers before closing netconf client?
            _client.close()
            client = None
            return True
        return False

    def _start_subscribers():
        for subscriber in subscribers.values():
            subscriber.start()

    def _stop_subscribers():
        for subscriber in subscribers.values():
            subscriber.stop()

    def rpc(content: xml.Node, add_rpc_attrs: list[(str, str)], callback: action(?xml.Node) -> None) -> None:
        if client is not None:
            # actonc: Acton/LambdaLifter.hs:(337,50)-(340,84): Non-exhaustive patterns in function attr
            # client.rpc(content, add_rpc_attrs, lambda _c, n: callback(n))
            def __callback(_c, n):
                callback(n)
            client.rpc(content, add_rpc_attrs, __callback)
        else:
            callback(None)

    def rpc_action(content: xml.Node, add_rpc_attrs: list[(str, str)], callback: action(?xml.Node) -> None) -> None:
        if client is not None:
            # actonc: Acton/LambdaLifter.hs:(337,50)-(340,84): Non-exhaustive patterns in function attr
            # client.rpc_action(content, add_rpc_attrs, lambda _c, n: callback(n))
            def __callback(_c, n):
                callback(n)
            client.rpc_action(content, add_rpc_attrs, __callback)
        else:
            callback(None)

    def close():
        _client = client
        if _client is not None:
            _client.close()
            client = None
        for subscriber in subscribers.values():
            subscriber.close()
        #subscribers.clear()
        subscribers = {}

def _source_params_try_append_dev_sub(source_params: TNode, dev_sub_key: Keypath):
    _device_name_key = dev_sub_key.try_get_key(0)
    if _device_name_key is not None:
        device_name = _device_name_key[0]
        source_params.leaf(None, PTag(None, 'device-name'), device_name)
    _sub_name_key = dev_sub_key.try_get_key(1)
    if _sub_name_key is not None:
        sub_name = _sub_name_key[0]
        source_params.leaf(None, PTag(None, 'subscription-name'), sub_name)

#actor NetconfRpcPollSubscription(source: NetconfSource, path: str, period_ms: int):
#actor NetconfRpcPollSubscription(source: NetconfSource, config: TNode, sinks: SubscriptionSinkCollection):
actor NetconfRpcPollSubscription(source: NetconfSource, dev_sub_key: Keypath, shared_schema: schema.SharedSchema, log_handler: ?logging.Handler):
    var logh = logging.Handler("netconf-rpc-poll-subscription")
    if log_handler is not None:
        logh.set_handler(log_handler)

    var log = logging.Logger(logh)

    _schema = schema.unsafe_get_shared_schema(shared_schema)
    sinks = SubscriptionSinkCollection()

    #var ned_id: ?HTag = HTag(710232548, 525139965) # ('http://tail-f.com/ns/ned-id/juniper-junos-nc-4.11', 'juniper-junos-nc-4.11')  # TODO: Receive NED-ID config
    var ned_id: ?HTag = HTag(817033024, 1122118695) # ('http://tail-f.com/ns/ned-id/ned-junos-23.1-yang-nc-1.0', 'ned-junos-23.1-yang-nc-1.0')  # TODO: Receive NED-ID config

    var poller_seqno: int = 0
    var rpc_xml: ?xml.Node = None
    var is_action: bool = False
    var schema_path: ?SchemaPath = None
    var period: time.Duration = time.Duration(60, 0, time.MonotonicClock(0, 0, 0))

    var is_running: bool = False

    log.info("SUBSCRIBER netconf-rpc-poll CREATED", None)

    def _do_rpc_request(seqno: int, poll_ts: time.Instant):
        if seqno != poller_seqno:
            return

        if rpc_xml is not None:
            request_ts = time.time()

            if is_action:
                source.rpc_action(rpc_xml, [], lambda r: _on_rpc_reply(seqno, poll_ts, request_ts, r))
            else:
                source.rpc(rpc_xml, [], lambda r: _on_rpc_reply(seqno, poll_ts, request_ts, r))

        #after 0.1: _on_rpc_reply(seqno, poll_ts, None)

    def _on_rpc_reply(seqno: int, last_ts: time.Instant, request_ts: time.Instant, node: ?xml.Node):
        if seqno != poller_seqno:
            return

        # # # <TEST>
        # # if node is None:
        # node = \
        #     xml.Node("rpc-reply", [(None, "urn:ietf:params:xml:ns:netconf:base:1.0")], None, [], [
        #         xml.Node("interface-information", [(None, "urn:juniper-rpc")], None, [], [
        #             xml.Node("physical-interface", [], None, [], [
        #                 xml.Node("name", [], None, [], [], "ge-0/0/0", None),
        #                 xml.Node("traffic-statistics", [], None, [], [
        #                     xml.Node("input-bytes", [], None, [], [], "12300", None),
        #                     xml.Node("output-bytes", [], None, [], [], "45600", None),
        #                     xml.Node("input-packets", [], None, [], [], "123", None),
        #                     xml.Node("output-packets", [], None, [], [], "456", None)
        #                 ], None, None)
        #             ], None, None)
        #         ], None, None)
        #     ], None, None)
        # # # </TEST>

        # TODO: Process node
        if node is not None:
            xnode = netconf.netconf_to_xnode(node, [], 0)
            log.debug("SUBSCRIBER netconf-rpc-poll POLL:", {"xnode": xnode})
            source_params = tnode_root()
            _source_params_try_append_dev_sub(source_params, dev_sub_key)
            source_params.leaf(None, PTag(None, 'timestamp'), request_ts)
            if schema_path is not None:
                source_params.leaf(None, PTag(None, 'schema_path'), schema_path)
            sinks.write(xnode, source_params)
        else:
            log.error("SUBSCRIBER netconf-rpc-poll POLL interrupted", None)

        #log.undecided("POLLED...", None)
        curr_ts = time.monotonic()
        next_ts = last_ts.add(period)
        #print("  period:", period.to_float())

        if next_ts < curr_ts:
            # Allow drift when reply arrived later than next period
            # TODO: Quantize to period?
            next_ts = curr_ts

        # print("  last_ts:", last_ts)
        # print("  curr_ts:", curr_ts)
        # print("  next_ts:", next_ts)

        delay = next_ts.since(curr_ts)
        log.debug("delay (s)", {"delay": delay.to_float()})
        after delay.to_float(): _do_rpc_request(seqno, next_ts)

    #def update_sinks(sink_updates: list[(Keypath, ?SubscriptionSink)]):
    def update_config(update: SubscriptionUpdate):
        sinks.update(update.sinks)

        _config = update.config
        if _config is not None:
            _stop_poller()
            log.debug("SUBSCRIBER netconf-rpc-poll CONFIG", {"config": _config})

            poll = _config[PTag('tlm', 'netconf-rpc-poll')]
            path = poll[PTag('tlm', 'path')].try_str()
            period_ms = poll[PTag('tlm', 'period')].try_int()

            if path is not None:
                # TODO: Proper parsing into ptag-/key-path
                #_tag = schema.QName.netconf_to_value(path, None)
                _tag = QName.netconf_to_value(path, None)
                if _tag is not None:
                    _update_rpc_params(Keypath([_tag]))

            if period_ms is not None and rpc_xml is not None:
                period = time.Duration(period_ms // 10**3, (period_ms % 10**3) * 10**15, time.MonotonicClock(0, 0, 0))
                #print("period: " + str(period.to_float()) + "s")
                if is_running:
                    _try_start_poller()
            else:
                pass # TODO: Indicate config error

    def _update_rpc_params(rpc_path: Keypath):
        _rpc_xml: ?xml.Node = None
        _is_action = False
        _schema_path: ?SchemaPath = None

        cursor = schema.Cursor(_schema)
        if cursor.push_schema_path(SchemaPath([PTag('ncs', 'devices'), PTag('ncs', 'device')], [])):
            is_cursor_ok = False
            rpc_path_len = len(rpc_path)
            if rpc_path_len == 1:
                # NED-RPC
                if cursor.push(PTag('ncs', 'rpc')):
                    if ned_id is not None and cursor.node().is_mount_point():
                        cursor.set_mount_id(ned_id)
                    unmangled_rpc_tag = rpc_path[0]
                    if isinstance(unmangled_rpc_tag, PTag):
                        prefix = unmangled_rpc_tag.prefix
                        name = unmangled_rpc_tag.name
                        if cursor.push(PTag(prefix, 'rpc-' + name)) and cursor.push(unmangled_rpc_tag):
                            is_cursor_ok = True
                        # if cursor.push(PTag(prefix, 'rpc-' + name)):
                        #     if cursor.push(unmangled_rpc_tag):
                        #         log.undecided("PING28", None)
                        #         is_cursor_ok = True
                        #     else:
                        #         print(optional_str(cursor.lookup_ptag(cursor.node().tag), str(cursor.node().tag)))
                        #         for c in cursor.node().children:
                        #             print("  " + optional_str(cursor.lookup_ptag(c), str(cursor.node().tag)))
                        # else:
                        #     print(optional_str(cursor.lookup_ptag(cursor.node().tag), str(cursor.node().tag)))
                        #     for c in cursor.node().children:
                        #         print("  " + optional_str(cursor.lookup_ptag(c), str(cursor.node().tag)))
            elif rpc_path_len > 1:
                # NED-ACTION
                if cursor.push(PTag('ncs', 'live-status')): # TODO: Any benefit to using PTag('ncs', 'config') instead? All actions are duplicated right?
                    if ned_id is not None and cursor.node().is_mount_point():
                        cursor.set_mount_id(ned_id)
                    is_cursor_ok = True
                    for _id in rpc_path:
                        if isinstance(_id, Tag) and not cursor.push(_id):
                            is_cursor_ok = False
                            break
                    _is_action = True

            if is_cursor_ok:
                rpc_itag = cursor.lookup_itag(cursor.node().tag)
                if rpc_itag is not None:
                    rpc_name = rpc_itag.name
                    ns = rpc_itag.ns
                    nsdefs = [(None, ns)] if ns is not None else []
                    _rpc_xml = xml.Node(rpc_itag.name, nsdefs, None, [], [], None, None)
                    _schema_path = cursor.get_schema_path()
                    # TODO: Configurable input params?

        rpc_xml = _rpc_xml
        is_action = _is_action
        schema_path = _schema_path

    def _try_start_poller():
        if rpc_xml is not None:
            curr_ts = time.monotonic()
            after 0: _do_rpc_request(poller_seqno, curr_ts)

    def _stop_poller():
        poller_seqno += 1

    def start():
        if not is_running:
            log.info("SUBSCRIBER netconf-rpc-poll STARTED", None)
            is_running = True
            _try_start_poller()

    def stop():
        if is_running:
            log.info("SUBSCRIBER netconf-rpc-poll STOPPED", None)
            is_running = False
            _stop_poller()

    def close():
        stop()

actor VManageSource(auth: WorldCap, shared_schema: schema.SharedSchema, log_handler: ?logging.Handler):
    var logh = logging.Handler("vmanage-source")
    if log_handler is not None:
        logh.set_handler(log_handler)

    var log = logging.Logger(logh)

    var config: TNode = tnode_empty()
    var client: ?vmanage.VManageHTTPClient = None
    var client_seqno: int = 0
    var subscribers: dict[Keypath, Subscriber] = {}

    log.info("SOURCE vmanage CREATED", None)

    def _on_client_connect(seqno: int):
        if seqno != client_seqno:
            return
        _start_subscribers()

    def _on_client_error(error_msg: str, seqno: int):
        if seqno != client_seqno:
            return
        _close_client()
        # TODO: Configurable retry timer/backoff?
        after 1: _connect_client()

    def _on_client_notif(node: xml.Node, seqno: int):
        # if seqno != client_seqno:
        #     return
        pass

    def update_config(source_update: ?TNode, subscription_updates: dict[Keypath, SubscriptionUpdate]):
        log.debug("SOURCE vmanage CONFIG update", {"source_update": optional_str(source_update, "None")})
        log.debug("SOURCE vmanage SUBSCRIPTION update", {"subsription_updates": mapping_str(subscription_updates)})

        restart = False

        if source_update is not None:
            if _close_client():
                restart = True

            config = source_update

            _connect_client()

        for dev_sub_key, sub_update in subscription_updates.items():
            if sub_update is not None:
                sub = try_get(subscribers, dev_sub_key)
                if sub is not None:
                    sub.update_config(sub_update)
                else:
                    new_sub = _create_subscriber(dev_sub_key, sub_update)
                    new_sub.update_config(sub_update)
                    if not restart and client is not None:
                        new_sub.start()
                    subscribers[dev_sub_key] = new_sub
            else:
                sub = try_pop(subscribers, dev_sub_key)
                if sub is not None:
                    sub.close()

        if restart:
            _start_subscribers()

    def _create_subscriber(dev_sub_key: Keypath, sub_update: SubscriptionUpdate) -> Subscriber:
        sub_config = sub_update.config
        if sub_config is not None:
            if sub_config[_SUBSCRIPTION_TYPE_VMANAGE_POLLER].exists():
                sub_act = VManagePollSubscription(self, dev_sub_key, shared_schema, logh)
                return Subscriber(sub_act.update_config, sub_act.start, sub_act.stop, sub_act.close)

        raise Exception("Broken invariant: Attempted to create subscriber from invalid config: " + optional_str(sub_config, "None"))

    def _connect_client():
        address = config[PTag('tlm', 'address')].try_str()
        port = config[PTag('tlm', 'port')].try_int()
        username = config[PTag('tlm', 'username')].try_str()
        password = config[PTag('tlm', 'password')].try_str()

        if address is not None and port is not None and \
                username is not None and password is not None:

            client_seqno += 1

            def __on_client_connect(_c):
                _on_client_connect(client_seqno)
            #def __on_client_error(_c, e):
            #    _on_client_error(e, client_seqno)

            tcpccap = net.TCPConnectCap(net.TCPCap(net.NetCap(auth)))
            new_client = vmanage.VManageHTTPClient(tcpccap, address, port, username, password, __on_client_connect, None)
            client = new_client

        # # <TEST>
        # client_seqno += 1
        # after 0: _on_client_connect(client_seqno)
        # # </TEST>

    def _close_client() -> bool:
        _client = client
        if _client is not None:
            _stop_subscribers()
            # TODO: Gather stop replies from subscribers before closing netconf client?
            _client.close()
            client = None
            return True
        return False

    def _start_subscribers():
        for subscriber in subscribers.values():
            subscriber.start()

    def _stop_subscribers():
        for subscriber in subscribers.values():
            subscriber.stop()

    def get(path: str, cb: action(?http.Response) -> None) -> None:
        if client is not None:
            def __callback(_c, n):
                cb(n)
            client.get(path, __callback)

    def close():
        _client = client
        if _client is not None:
            _client.close()
            client = None
        for subscriber in subscribers.values():
            subscriber.close()
        #subscribers.clear()
        subscribers = {}


actor VManagePollSubscription(source: VManageSource, dev_sub_key: Keypath, shared_schema: schema.SharedSchema, log_handler: logging.Handler):
    var logh = logging.Handler("vmanage-poll-subscription")
    if log_handler is not None:
        logh.set_handler(log_handler)

    var log = logging.Logger(logh)

    _schema = schema.unsafe_get_shared_schema(shared_schema)
    sinks = SubscriptionSinkCollection()

    var poller_seqno: int = 0
    var poll_path: ?str = None
    var schema_path: ?SchemaPath = None
    var period: time.Duration = time.Duration(60, 0, time.MonotonicClock(0, 0, 0))

    var is_running: bool = False

    log.info("SUBSCRIBER vmanage CREATED", None)

    def _do_poll(seqno: int, poll_ts: time.Instant):
        if seqno != poller_seqno:
            return

        if poll_path is not None:
            request_ts = time.time()
            url = "/dataservice" + poll_path
            source.get("/dataservice" + poll_path, lambda r: _on_poll_reply(seqno, poll_ts, request_ts, r))
            #try:
            #    source.get("/dataservice" + poll_path, lambda r: _on_poll_reply(seqno, poll_ts, request_ts, r))
            #except vmanage.NoConnError:
            #    after 1.0: _do_poll(seqno, next_ts)

            # # <TEST>
            # body = """{
            # "header": {
            #     "generatedOn": 1695290859047
            # },
            # "total_records": 2,
            # "data": [
            #     {
            #       "latency": 5,
            #       "dst_ip": "10.1.3.1",
            #       "src_ip": "10.1.1.1",
            #       "siteid": 228,
            #       "loss_percentage": 0,
            #       "jitter": 8,
            #       "name": "10.100.1.1:public-internet-10.100.3.1:public-internet",
            #       "host_name": "de-bsp-aachen-ce-3"
            #     },
            #     {
            #       "latency": 6,
            #       "dst_ip": "10.1.1.1",
            #       "src_ip": "10.1.3.1",
            #       "siteid": 230,
            #       "loss_percentage": 1.3,
            #       "jitter": 9,
            #       "name": "10.100.3.1:public-internet-10.100.1.1:public-internet",
            #       "host_name": "de-bsp-chemni-ce-2"
            #     }
            # ]
            # }""".encode()

            # response = http.Response(bytes([]), 200, {}, body)
            # after 0: _on_poll_reply(seqno, poll_ts, request_ts, response)
            # # </TEST>


    def _on_poll_reply(seqno: int, last_ts: time.Instant, request_ts: time.Instant, reply: ?http.Response):
        if seqno != poller_seqno:
            return

        if reply is not None:
            try:
                ar = reply.decode_json()
                ar_data = ar["data"]
                if isinstance(ar_data, list):
                    xnode = vmanage.approute_to_xnode(ar_data)
                    source_params = tnode_root()
                    _source_params_try_append_dev_sub(source_params, dev_sub_key)
                    source_params.leaf(None, PTag(None, 'timestamp'), request_ts)
                    if schema_path is not None:
                        source_params.leaf(None, PTag(None, 'schema_path'), schema_path)
                    sinks.write(xnode, source_params)
            except Exception as exc:
                log.error("Exception", {"exception": exc})
                log.debug("Reply body", {"body": reply.body})

        _schedule_next_poll(seqno, last_ts)

    def _schedule_next_poll(seqno: int, last_ts: time.Instant):
        curr_ts = time.monotonic()
        next_ts = last_ts.add(period)

        if next_ts < curr_ts:
            # Allow drift when reply arrived later than next period
            # TODO: Quantize to period?
            next_ts = curr_ts

        # print("  last_ts:", last_ts)
        # print("  curr_ts:", curr_ts)
        # print("  next_ts:", next_ts)

        delay = next_ts.since(curr_ts)
        after delay.to_float(): _do_poll(seqno, next_ts)

    def update_config(update: SubscriptionUpdate):
        sinks.update(update.sinks)

        _config = update.config
        if _config is not None:
            _stop_poller()

            poll = _config[PTag('tlm', 'vmanage-poll')]
            path = poll[PTag('tlm', 'path')].try_str()
            period_ms = poll[PTag('tlm', 'period')].try_int()

            if path is not None:
                poll_path = path
                # TODO: Proper parsing into ptag-/key-path
                #_tag = schema.QName.netconf_to_value(path, None)
                _tag = QName.netconf_to_value(path, None)
                if _tag is not None:
                    _update_schema_cursor(Keypath([_tag]))

            if period_ms is not None and poll_path is not None:
                period = time.Duration(period_ms // 10**3, (period_ms % 10**3) * 10**15, time.MonotonicClock(0, 0, 0))
                if is_running:
                    _try_start_poller()

    def _update_schema_cursor(kpath: Keypath):
        cursor = schema.Cursor(_schema)
        if cursor.push_schema_path(SchemaPath([PTag('ncs', 'devices'), PTag('ncs', 'device')], [])):
            if cursor.push(PTag('ncs', 'live-status')): # TODO: Any benefit to using PTag('ncs', 'config') instead? All actions are duplicated right?
                schema_path = cursor.get_schema_path()

    def _try_start_poller():
        if poll_path is not None:
            curr_ts = time.monotonic()
            after 0: _do_poll(poller_seqno, curr_ts)

    def _stop_poller():
        poller_seqno += 1

    def start():
        if not is_running:
            log.info("SUBSCRIBER vmanage-poll STARTED", None)
            is_running = True
            _try_start_poller()

    def stop():
        if is_running:
            log.info("SUBSCRIBER vmanage-poll STOPPED", None)
            is_running = False
            _stop_poller()

    def close():
        stop()

actor NetconfGetPollSubscription(source: NetconfSource, dev_sub_key: Keypath, shared_schema: schema.SharedSchema, log_handler: ?logging.Handler):
    var logh = logging.Handler("netconf-get-poll")
    if log_handler is not None:
        logh.set_handler(log_handler)

    var log = logging.Logger(logh)

    _schema = schema.unsafe_get_shared_schema(shared_schema)
    sinks = SubscriptionSinkCollection()

    #var ned_id: ?HTag = HTag(710232548, 525139965) # ('http://tail-f.com/ns/ned-id/juniper-junos-nc-4.11', 'http://tail-f.com/ns/ned-id/juniper-junos-nc-4.11')  # TODO: Receive NED-ID config
    var ned_id: ?HTag = HTag(1503593780, 92759342) # ('http://tail-f.com/ns/ned-id/ned-ios-xe-yang-nc-17.11', 'ned-ios-xe-yang-nc-17.11')  # TODO: Receive NED-ID config

    var poller_seqno: int = 0
    var get_xml: ?xml.Node = None
    var schema_path: ?SchemaPath = None
    var period: time.Duration = time.Duration(60, 0, time.MonotonicClock(0, 0, 0))

    var is_running: bool = False

    log.info("SUBSCRIBER netconf-get-poll CREATED", None)

    def _do_get_request(seqno: int, poll_ts: time.Instant):
        log.debug("SUBSCRIBER netconf-get-poll GET_REQUEST", None)
        if seqno != poller_seqno:
            return

        log.trace("SUBSCRIBER netconf-get-poll GET_XML", {"get_xml": optional_str(get_xml, "None")})
        if get_xml is not None:
            request_ts = time.time()

            source.rpc(get_xml, [], lambda r: _on_get_reply(seqno, poll_ts, request_ts, r))

            # # <TEST>
            # node = xml.decode("""
            #     <rpc-reply message-id="1" xmlns="urn:ietf:params:xml:ns:netconf:base:1.0">
            #       <data>
            #         <ip-sla-stats xmlns="http://cisco.com/ns/yang/Cisco-IOS-XE-ip-sla-oper">
            #           <sla-oper-entry>
            #             <oper-id>167837954</oper-id>
            #             <stats>
            #               <oneway-latency>
            #                 <sample-count>100</sample-count>
            #                 <sd>
            #                   <min>7</min>
            #                   <avg>8</avg>
            #                   <max>11</max>
            #                   <accuracy>accuracy-milliseconds</accuracy>
            #                 </sd>
            #                 <ds>
            #                   <min>2</min>
            #                   <avg>13</avg>
            #                   <max>35</max>
            #                   <accuracy>accuracy-milliseconds</accuracy>
            #                 </ds>
            #               </oneway-latency>
            #               <jitter>
            #                 <sd-sample-count>100</sd-sample-count>
            #                 <ds-sample-count>100</ds-sample-count>
            #                 <sd>
            #                   <min>2</min>
            #                   <avg>3</avg>
            #                   <max>4</max>
            #                   <accuracy>accuracy-milliseconds</accuracy>
            #                 </sd>
            #                 <ds>
            #                   <min>3</min>
            #                   <avg>4</avg>
            #                   <max>5</max>
            #                   <accuracy>accuracy-milliseconds</accuracy>
            #                 </ds>
            #               </jitter>
            #               <packet-loss>
            #                 <sd-count>1</sd-count>
            #                 <ds-count>2</ds-count>
            #               </packet-loss>
            #             </stats>
            #           </sla-oper-entry>
            #           <sla-oper-entry>
            #             <oper-id>167838210</oper-id>
            #             <stats>
            #               <oneway-latency>
            #                 <sample-count>200</sample-count>
            #                 <sd>
            #                   <min>4</min>
            #                   <avg>5</avg>
            #                   <max>6</max>
            #                   <accuracy>accuracy-milliseconds</accuracy>
            #                 </sd>
            #                 <ds>
            #                   <min>5</min>
            #                   <avg>6</avg>
            #                   <max>7</max>
            #                   <accuracy>accuracy-milliseconds</accuracy>
            #                 </ds>
            #               </oneway-latency>
            #               <jitter>
            #                 <sd-sample-count>200</sd-sample-count>
            #                 <ds-sample-count>200</ds-sample-count>
            #                 <sd>
            #                   <min>1</min>
            #                   <avg>2</avg>
            #                   <max>3</max>
            #                   <accuracy>accuracy-milliseconds</accuracy>
            #                 </sd>
            #                 <ds>
            #                   <min>2</min>
            #                   <avg>3</avg>
            #                   <max>4</max>
            #                   <accuracy>accuracy-milliseconds</accuracy>
            #                 </ds>
            #               </jitter>
            #               <packet-loss>
            #                 <sd-count>5</sd-count>
            #                 <ds-count>7</ds-count>
            #               </packet-loss>
            #             </stats>
            #           </sla-oper-entry>
            #           <sla-oper-error-statistics>
            #             <oper-id>167837954</oper-id>
            #             <target-address>10.1.1.2</target-address>
            #           </sla-oper-error-statistics>
            #           <sla-oper-error-statistics>
            #             <oper-id>167838210</oper-id>
            #             <target-address>10.1.2.2</target-address>
            #           </sla-oper-error-statistics>
            #         </ip-sla-stats>
            #       </data>
            #     </rpc-reply>
            #     """)

            # after 0: _on_get_reply(seqno, poll_ts, request_ts, node)
            # # </TEST>

    def _on_get_reply(seqno: int, last_ts: time.Instant, request_ts: time.Instant, node: ?xml.Node):
        log.debug("SUBSCRIBER netconf-get-poll GET_REPLY", None)
        if seqno != poller_seqno:
            return

        # TODO: Process node
        if node is not None:
            if node.tag == "rpc-reply":
                 node = node.children[0]
                 if node.tag == "data" and len(node.children) >= 1:
                     node = node.children[0]
                     xnode = netconf.netconf_to_xnode(node, [], 0)
                     log.trace("SUBSCRIBER netconf-get-poll POLL:", {"xnode": xnode})
                     source_params = tnode_root()
                     _source_params_try_append_dev_sub(source_params, dev_sub_key)
                     source_params.leaf(None, PTag(None, 'timestamp'), request_ts)
                     if schema_path is not None:
                         source_params.leaf(None, PTag(None, 'schema_path'), schema_path)
                     sinks.write(xnode, source_params)
        else:
            pass
            log.error("SUBSCRIBER netconf-get-poll POLL interrupted", None)

        curr_ts = time.monotonic()
        next_ts = last_ts.add(period)

        if next_ts < curr_ts:
            # Allow drift when reply arrived later than next period
            # TODO: Quantize to period?
            next_ts = curr_ts

        # log.info("  last_ts:", last_ts, None)
        # log.info("  curr_ts:", curr_ts, None)
        # log.info("  next_ts:", next_ts, None)

        delay = next_ts.since(curr_ts)
        log.debug("  ... delay " + str(delay.to_float()) + "s", None)
        after delay.to_float(): _do_get_request(seqno, next_ts)

    #def update_sinks(sink_updates: list[(Keypath, ?SubscriptionSink)]):
    def update_config(update: SubscriptionUpdate):
        sinks.update(update.sinks)

        _config = update.config
        if _config is not None:
            _stop_poller()
            log.debug("SUBSCRIBER netconf-get-poll CONFIG", {"_config": _config})

            poll = _config[PTag('tlm', 'netconf-get-poll')]
            path = poll[PTag('tlm', 'path')].try_str()
            period_ms = poll[PTag('tlm', 'period')].try_int()

            if path is not None:
                # TODO: Proper parsing into ptag-/key-path
                #_tag = schema.QName.netconf_to_value(path, None)
                _tag = QName.netconf_to_value(path, None)
                if _tag is not None:
                    _update_get_params(Keypath([_tag]))

            if period_ms is not None and get_xml is not None:
                period = time.Duration(period_ms // 10**3, (period_ms % 10**3) * 10**15, time.MonotonicClock(0, 0, 0))
                log.debug("period: " + str(period.to_float()) + "s", None)
                if is_running:
                    _try_start_poller()
            else:
                pass # TODO: Indicate config error

    def _update_get_params(get_path: Keypath):
        _get_xml: ?xml.Node = None
        _is_action = False
        _schema_path: ?SchemaPath = None

        cursor = schema.Cursor(_schema)
        if cursor.push_schema_path(SchemaPath([PTag('ncs', 'devices'), PTag('ncs', 'device')], [])):
            is_cursor_ok = False
            _get_path: list[(str, list[(?str,str)])] = []

            # NED-GET
            if cursor.push(PTag('ncs', 'live-status')):
                if ned_id is not None and cursor.node().is_mount_point():
                    cursor.set_mount_id(ned_id)
                is_cursor_ok = True
                for _id in get_path:
                    if isinstance(_id, Tag) and not cursor.push(_id):
                        # TODO: print some error or something?
                        log.error("Invalid subscription path, bad node:", {"_id": _id})
                        is_cursor_ok = False
                        break

                    get_itag = cursor.lookup_itag(cursor.node().tag)
                    if get_itag is not None:
                        _ns: ?str = get_itag.ns
                        if _ns is not None:
                            _nsdef: (?str,str) = (None, _ns)
                            _get_path.append((get_itag.name, [_nsdef]))
                        else:
                            _get_path.append((get_itag.name, []))

            if is_cursor_ok:
                get_itag = cursor.lookup_itag(cursor.node().tag)
                if get_itag is not None:
                    get_name = get_itag.name
                    ns = get_itag.ns
                    nsdefs = [(None, ns)] if ns is not None else []
                    _filter_node: ?xml.Node = None
                    if len(_get_path) > 0:
                        for _itag in reversed(_get_path):
                            if _filter_node is not None:
                                _children = [_filter_node]
                            else:
                                _children = []

                            _filter_node = xml.Node(_itag.0, _itag.1, None, [], _children, None, None)

                        if _filter_node is not None:
                            _filter_node = xml.Node("filter", [], None, [], [_filter_node], None, None)

                    if _filter_node is not None:
                        _get_xml = xml.Node("get", [], None, [], [_filter_node], None, None)
                    else:
                        # TODO: this shoule be some form of error, right?
                        _get_xml = xml.Node("get", [], None, [], [], None, None)
                    _schema_path = cursor.get_schema_path()
                    # TODO: Configurable input params?

        get_xml = _get_xml
        schema_path = _schema_path

    def _try_start_poller():
        log.debug("SUBSCRIBER netconf-get-poll TRY_START", None)
        log.debug("SUBSCRIBER netconf-get-poll GET_XML", {"get_xml": optional_str(get_xml, "None")})
        if get_xml is not None:
            curr_ts = time.monotonic()
            # TODO: reduce to after 0. But we're currently getting an error, the
            # SSH client process is closed with exit code 7 with a delay 0
            # SSH process exited with code: 7  and termination signal: 0
            after 5: _do_get_request(poller_seqno, curr_ts)

    def _stop_poller():
        poller_seqno += 1

    def start():
        if not is_running:
            log.info("SUBSCRIBER netconf-get-poll STARTED", None)
            is_running = True
            _try_start_poller()

    def stop():
        if is_running:
            log.info("SUBSCRIBER netconf-get-poll STOPPED", None)
            is_running = False
            _stop_poller()

    def close():
        pass

class SubscriptionUpdate(object):
    @property
    config: ?TNode
    @property
    sinks: list[(Keypath, ?SubscriptionSink)]

    def __init__(self):
        self.config = None
        self.sinks = []

    def __str__(self) -> str:
        s = []
        for sink_key, sink in self.sinks:
            unsafe_list_append(s, "(" + str(sink_key) + ", " + optional_str(sink, "None") + ")")
        return "SubscriptionUpdate(" + optional_str(self.config, "None") + ", " + str(s) + ")"

actor DeviceStreamer(env: Env, name: str, shared_schema: schema.SharedSchema, shared_resources: SharedResources, log_handler: ?logging.Handler):
    var logh = logging.Handler("device-streamer")
    if log_handler is not None:
        logh.set_handler(log_handler)

    var log = logging.Logger(logh)

    var sources: dict[Keypath, Source] = {}
    var subscription_sources: dict[Keypath, Keypath] = {}
    var sinks: dict[Keypath, Sink] = {}

    log.info("DEVICE CREATED", {"name": name})

    def on_config(
            updated_device_config: ?DeviceStreamerConfig,
            subscription_updates: list[(Keypath, ?value)],
            subscription_sink_updates: list[(Keypath, ?value)],
            source_updates: list[(Keypath, ?value)],
            sink_updates: list[(Keypath, ?value)]):

        log.debug("DEVICE CONFIG", {"name": name, "updated_device_config": optional_str(updated_device_config, "None")})

        _subscription_updates = {}
        for k, v in subscription_updates:
            _subscription_updates[str(k)] = str(v) if v is not None else "DELETED"
        log.trace("SUBSCRIPTIONS", _subscription_updates)

        _subscription_sink_updates = {}
        for k, v in subscription_sink_updates:
            _subscription_sink_updates[str(k)] = str(v) if v is not None else "DELETED"
        log.trace("SUBSCRIPTION SINKS", _subscription_sink_updates )

        _source_updates = {}
        for k, v in source_updates:
            _source_updates[str(k)] = str(v) if v is not None else "DELETED"
        log.trace("SOURCES", _source_updates)

        _sink_updates = {}
        for k, v in sink_updates:
            _sink_updates[str(k)] = str(v) if v is not None else "DELETED"
        log.trace("SINKS", _sink_updates)

        source_config_updates: dict[Keypath, TNode] = {}
        updated_source_dev_subs: dict[Keypath, dict[Keypath, SubscriptionUpdate]] = {}

        deleted_dev_sub_keys: set[Keypath] = set([])

        # TODO:
        # Merge device_config and source_config
        # but only using source_config for a while...

        for source_key, state in source_updates:
            source_config_updates[source_key] = state if state is not None and isinstance(state, TNode) else tnode_empty()
            get_or_create(updated_source_dev_subs, source_key, lambda: {})

        if sink_updates:
            for sink_key, state in sink_updates:
                _update_sink(sink_key, state)

        for dev_sub_key, state in subscription_updates:
            if state is not None and isinstance(state, TNode):
                source_key = _get_subscription_source_key(state)
                subscription_sources[dev_sub_key] = source_key
                sub_update = get_or_create(get_or_create(updated_source_dev_subs, source_key, lambda: {}), dev_sub_key, SubscriptionUpdate)
                sub_update.config = state
            else:
                deleted_dev_sub_keys.add(dev_sub_key)

        for dev_sub_sink_key, state in subscription_sink_updates:
            dev_sub_key = dev_sub_sink_key.try_slice(0, 2)
            sink_key = dev_sub_sink_key.try_slice(2, 3)
            if dev_sub_key is not None and sink_key is not None:
                source_key = try_get(subscription_sources, dev_sub_key)
                if source_key is not None:
                    sub_update = get_or_create(get_or_create(updated_source_dev_subs, source_key, lambda: {}), dev_sub_key, SubscriptionUpdate)
                    sub_sink: ?SubscriptionSink = None
                    if state is not None and isinstance(state, TNode):
                        sub_sink = _create_subscription_sink(sink_key, state)
                    sub_update.sinks.append((sink_key, sub_sink))

        for dev_sub_key in deleted_dev_sub_keys:
            source_key = try_pop(subscription_sources, dev_sub_key)
            if source_key is not None:
                # sub_update = get_or_create(get_or_create(updated_source_dev_subs, source_key, lambda: {}), dev_sub_key, SubscriptionUpdate)
                # sub_update.config = None
                get_or_create(get_or_create(updated_source_dev_subs, source_key, lambda: {}), dev_sub_key, SubscriptionUpdate)

        for source_key, sub_updates in updated_source_dev_subs.items():
            source = _get_or_create_source(source_key)
            source.update_config(try_get(source_config_updates, source_key), sub_updates)
            if not source.has_subscriptions():
                source.close()
                del sources[source_key]

    def _get_subscription_source_key(state: TNode) -> Keypath:
        if state[_SUBSCRIPTION_TYPE_ACTION_POLLER].exists():
            return SOURCE_KEY_NETCONF
        if state[_SUBSCRIPTION_TYPE_GET_POLLER].exists():
            return SOURCE_KEY_NETCONF
        if state[_SUBSCRIPTION_TYPE_VMANAGE_POLLER].exists():
            return SOURCE_KEY_VMANAGE_HTTP

        raise Exception("Unknown subscription type: " + str(state))

    def _get_or_create_source(source_key: Keypath) -> Source:
        _source = try_get(sources, source_key)
        if _source is not None:
            return _source

        if source_key == SOURCE_KEY_NETCONF:
            source_act = NetconfSource(env.cap, shared_schema, logh)
            source = Source(source_key, source_act.update_config, source_act.close)
            sources[source_key] = source
            return source

        if source_key == SOURCE_KEY_VMANAGE_HTTP:
            source_act = VManageSource(env.cap, shared_schema, logh)
            source = Source(source_key, source_act.update_config, source_act.close)
            sources[source_key] = source
            return source

        raise Exception("Unknown source type: " + str(source_key))

    def _update_sink(sink_key: Keypath, config: ?value):
        old_sink = try_pop(sinks, sink_key)
        if old_sink is not None:
            old_sink.close()

        if config is not None and isinstance(config, TNode):
            if config[PTag('tlm', 'm3db')].exists():
                sink_act = M3dbSink(env.cap, config, shared_schema, logh)
                sinks[sink_key] = Sink(sink_act.write, sink_act.close)
                return
            # elif config[PTag('tlm', 'nso-data-provider')].exists():
            #     ...
            #     sinks[sink_key] = sink
            #     return
            elif config[PTag('tlm', 'nso-cdb')].exists():
                sink_act = NsoCdbSink(env, config, shared_schema)
                sinks[sink_key] = Sink(sink_act.write, sink_act.close)
                return

    def _create_subscription_sink(sink_key: Keypath, config: TNode) -> ?SubscriptionSink:
        sink: ?Sink = try_get(sinks, sink_key)

        if sink is not None:
            head_cb: action(Node, TNode) -> None = action lambda n, p: sink.write(n, p, config)

            transforms: list[Transform] = []
            # TODO: Make sure we get elements as 'ordered-by user' from CdbCache/TNode.
            for _transform_elem in reversed(list(config[PTag('tlm', 'transform')])):
                transform = _create_transform(_transform_elem, head_cb)
                if transform is not None:
                    transforms.append(transform)
                    head_cb = transform.post # head_cb = transform.post_fn
                else:
                    pass
                    # TODO
                    # raise Exception("Unknown transform type: " + str(_transform_elem))

            return SubscriptionSink(config, sink, transforms, head_cb)

        return None

    def _create_transform(config: TNode, next_cb: action(Node, TNode) -> None) -> ?Transform:
        # Magic name string for hardcoded transform for now... i.e. demo.
        if eq_optional(config[PTag('tlm', 'name')].try_str(), "vmanage"):
            transform_act = telemetrify.vmanage.transform.Transform(
                next_cb,
                shared_resources.request_l3vpn_svc_tracker,
                shared_resources.release_l3vpn_svc_tracker)
            return Transform(transform_act.transform, transform_act.close)
        elif eq_optional(config[PTag('tlm', 'name')].try_str(), "cat8k-ip-sla"):
            transform_act = telemetrify.ietf.cat8k.IpSlaTransform(
                next_cb,
                shared_resources.request_l3vpn_svc_tracker,
                shared_resources.release_l3vpn_svc_tracker)
            return Transform(transform_act.transform, transform_act.close)
        return None

    def close() -> None:
        for source in sources.values():
            source.close()
        #sources.clear()
        sources = {}
        for sink in sinks.values():
            sink.close()
        #sinks.clear()
        sinks = {}

actor SharedResources(env: Env, shared_schema: schema.SharedSchema):
    var l3vpn_svc_tracker: ?telemetrify.ietf.l3vpn_svc.L3vpnSvcTracker = None

    # def request_l3vpn_svc_tracker(cb: action(telemetrify.ietf.l3vpn_svc.L3vpnSvcTracker) -> None):
    # TODO: Why do we get this error when there is MORE than one reference to the method, but NOT if there is ONLY one?
    # ERROR: Error when compiling telemetrify.main.main module: Type error
    #  1425:9-33
    #      |
    # 1425 |    def request_l3vpn_svc_tracker(cb: action(telemetrify.ietf.l3vpn_svc.L3vpnSvcTracker) -> None):
    #      |        ^^^^^^^^^^^^^^^^^^^^^^^^^
    # NoInfo (Loc 58253 58278) 63
    def request_l3vpn_svc_tracker(cb):
        r = l3vpn_svc_tracker
        if r is not None:
            cb(r)
        else:
            n = telemetrify.ietf.l3vpn_svc.L3vpnSvcTracker(env, shared_schema)
            l3vpn_svc_tracker = n
            cb(n)

    def release_l3vpn_svc_tracker(tracker: telemetrify.ietf.l3vpn_svc.L3vpnSvcTracker):
        pass # TODO: Could keep refcount and release when not used

actor main(env):
    var device_streamers: dict[Keypath, DeviceStreamer] = {}
    var cache: ?telemetrify.nso.subscriber.CdbCache = None
    var shared_resources: ?SharedResources = None

    var logh = logging.Handler("telemetrify")
    var log = logging.Logger(logh)

    def _on_maapi_connect_error(e):
        log.error("MAAPI connect failed:" + str(e), None)

    def _on_maapi_connect(c):
        log.info("MAAPI connected!!!!", None)
        c.load_schema(_on_load_schema)

    def _on_load_schema(c, e: ?Exception, shared_schema: ?schema.SharedSchema):
        if e is None and shared_schema is not None:
            log.info("MAAPI loaded schema!!!!", None)
            shared_resources = SharedResources(env, shared_schema)
            cdb_connection = telemetrify.nsoapi.cdb.CdbConnection(env, 4569, "telemetrify",
                action lambda c: _on_cdb_sub_connect(c, shared_schema), _on_cdb_connect_error, None)
        else:
            log.error("MAAPI load schema failed:" + optional_str(e, ""), None)
            await async env.exit(1)

    def _on_cdb_connect_error(e):
        log.error("CDB connect failed:" + str(e), None)
        await async env.exit(1)

    def _on_cdb_sub_connect(sc, s):
        log.info("CDB (sub) connected!!!!", None)
        cdb_connection = telemetrify.nsoapi.cdb.CdbConnection(env, 4569, "telemetrify",
            action lambda cc: _on_cdb_cmd_connect(sc, cc, s), _on_cdb_connect_error, None)

    def _on_cdb_cmd_connect(sc, cc, s):
        log.info("CDB (cmd) connected!!!!", None)
        cache = telemetrify.nso.subscriber.CdbCache(sc, cc, s,
            [
                DeviceSettingsRefiner,
                DeviceSourceNetconfRefiner,
                DeviceSourceVmanageRefiner,
                DeviceSourceRefiner,
                DeviceStreamerRefiner,
                DeviceSubscriptionSourceRefiner,
                DeviceSubscriptionSinkBaseRefiner,
                DeviceSubscriptionSinkRefiner,
                SinkRefiner,
                DeviceSinksRefiner,
            ],
            [([
                #DeviceSettingsRefiner.id(), # For DEBUG printouts only
                DeviceStreamerRefiner.id(),
                DeviceSourceNetconfRefiner.id(), # For DEBUG printouts only
                DeviceSourceVmanageRefiner.id(), # For DEBUG printouts only
                DeviceSourceRefiner.id(),
                DeviceSubscriptionSourceRefiner.id(),
                #DeviceSubscriptionSinkBaseRefiner.id(), # For DEBUG printouts purposes
                DeviceSubscriptionSinkRefiner.id(),
                #SinkRefiner.id(), # For DEBUG printouts purposes
                DeviceSinksRefiner.id()
            ],
            lambda r: _on_config_update(r, s))], _on_config_cache_error)
        #await async env.exit(0)

    def _on_config_update(refiner_updates: dict[int, list[(Keypath, ?value)]], shared_schema: schema.SharedSchema):
        for refiner_id, updates in refiner_updates.items():
            print("refiner_id: " + str(refiner_id))
            for k, v in updates:
                print("  " + str(k) + ": " + (str(v) if v is not None else "DELETED"))

        dev_config: list[(Keypath, ?value)] = refiner_updates[DeviceStreamerRefiner.id()]
        sub_config: list[(Keypath, ?value)] = refiner_updates[DeviceSubscriptionSourceRefiner.id()]
        sub_sink_config: list[(Keypath, ?value)] = refiner_updates[DeviceSubscriptionSinkRefiner.id()]
        source_config: list[(Keypath, ?value)] = refiner_updates[DeviceSourceRefiner.id()]
        # sink_config: list[(Keypath, ?value)] = refiner_updates[SinkRefiner.id()]
        sink_config: list[(Keypath, ?value)] = refiner_updates[DeviceSinksRefiner.id()]

        updated_device_keys: set[Keypath] = set([])

        updated_subscriptions: dict[Keypath, list[(Keypath, ?value)]] = {}
        updated_subscription_sinks: dict[Keypath, list[(Keypath, ?value)]] = {}
        updated_device_sources: dict[Keypath, list[(Keypath, ?value)]] = {}
        updated_device_sinks: dict[Keypath, list[(Keypath, ?value)]] = {}
        updated_device_configs: dict[Keypath, DeviceStreamerConfig] = {}

        for dev_sub_key, state in sub_config:
            device_key = dev_sub_key.try_slice(0, 1)
            if device_key is not None:
                dict_list_append(updated_subscriptions, device_key, (dev_sub_key, state))
                updated_device_keys.add(device_key)

        for dev_sub_sink_key, state in sub_sink_config:
            device_key = dev_sub_sink_key.try_slice(0, 1)
            if device_key is not None:
                dict_list_append(updated_subscription_sinks, device_key, (dev_sub_sink_key, state))
                updated_device_keys.add(device_key)

        for device_key, state in dev_config:
            if state is not None and isinstance(state, DeviceStreamerConfig):
                entry = try_get(device_streamers, device_key)
                if entry is None:
                    _device_name_key = device_key.try_get_key(0)
                    if _device_name_key is not None:
                        device_name = _device_name_key[0]
                        _shared_resources = shared_resources
                        if isinstance(device_name, str) and _shared_resources is not None:
                            device_streamers[device_key] = DeviceStreamer(env, device_name, shared_schema, _shared_resources, None)
                updated_device_keys.add(device_key)
                updated_device_configs[device_key] = state
            else:
                _remove_device_streamer(device_key)
                updated_device_keys.discard(device_key)

        for device_source_key, state in source_config:
            device_key = device_source_key.try_slice(0, 1)
            source_key = device_source_key.try_slice(1, 2)
            if device_key is not None and source_key is not None:
                dict_list_append(updated_device_sources, device_key, (source_key, state))
                updated_device_keys.add(device_key)

        for device_sink_key, state in sink_config:
            device_key = device_sink_key.try_slice(0, 1)
            sink_key = device_sink_key.try_slice(1, 2)
            if device_key is not None and sink_key is not None:
                dict_list_append(updated_device_sinks, device_key, (sink_key, state))
                updated_device_keys.add(device_key)

        for updated_device_key in updated_device_keys:
            entry = try_get(device_streamers, updated_device_key)
            if entry is not None:
                entry.on_config(
                    try_get(updated_device_configs, updated_device_key),
                    updated_subscriptions.get(updated_device_key, []),
                    updated_subscription_sinks.get(updated_device_key, []),
                    updated_device_sources.get(updated_device_key, []),
                    updated_device_sinks.get(updated_device_key, []))

    def _remove_device_streamer(device_key):
        entry = try_pop(device_streamers, device_key)
        if entry is not None:
            entry.close()

    def _on_config_cache_error(e):
        log.error("CdbCache failed:" + optional_str(e.error_message, ""), None)
        await async env.exit(1)

    log.info("Starting up...", None)
    maapi_connection = telemetrify.nsoapi.maapi.MaapiConnection(env, 4569, _on_maapi_connect, _on_maapi_connect_error, None)
